{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"REF: https://github.com/GoogleCloudPlatform/df-ml-anomaly-detection/blob/master/README.md\">Reference1<a/>  <br>\n",
    "<a href=\"https://faker.readthedocs.io/en/master/\">Reference2<a/> <br>\n",
    "<a href=\"https://huggingface.co/docs/datasets/v1.11.0/processing.html#exporting-a-dataset-to-csv-json-parquet-or-to-python-objects\">Reference3<a/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data.iloc[index, :]\n",
    "        return sample\n",
    "    \n",
    "class CustomDataCollator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        batch = torch.tensor(batch, dtype=torch.float32)\n",
    "        return batch\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.mu_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.sigma_layer = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        mean = self.mu_layer(x)\n",
    "        log_var = self.sigma_layer(x)\n",
    "        return mean, log_var\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "class VAEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.enc_layer = Encoder(input_dim, hidden_dim, latent_dim)\n",
    "        self.dec_layer = Decoder(latent_dim, hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean, log_var = self.enc_layer(x)\n",
    "        z = self.reparameterize(mean, log_var)\n",
    "        x = self.dec_layer(z)\n",
    "        return x , mean, log_var\n",
    "    \n",
    "    def reparameterize(self, mean, log_var):\n",
    "        eps = torch.randn_like(mean, requires_grad=False)\n",
    "        z = mean + torch.exp(0.5 * log_var) * eps\n",
    "        return z\n",
    "\n",
    "def network_loss(x, x_hat, mean, log_var):\n",
    "    r_loss = F.mse_loss(x_hat, x, reduction='mean')\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())\n",
    "    score = r_loss + kl_loss\n",
    "    return score\n",
    "\n",
    "class DetectAnomaly:\n",
    "    def __init__(self, loss):\n",
    "        self.mean = torch.mean(torch.stack(loss)).item()\n",
    "        self.std_dev = torch.mean(torch.stack(loss)).item()\n",
    "        self.threshold()\n",
    "        \n",
    "    def threshold(self):\n",
    "        self.threshold = self.mean + 3 * self.std_dev\n",
    "        \n",
    "    def chekcer(self, loss):\n",
    "        if loss > self.threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(\n",
    "    path_or_buf ='/Users/hardey/Desktop/GITHUB/AnomalyDetectionPipeline/data/-2023-10-16T10:25:00-2023-10-16T10:26:00-00000-of-00001',\n",
    "    orient='record',\n",
    "    lines=True)\n",
    "data.drop(columns=['subscriberId','dstIP','UniqueIPs','UniquePorts','NumRecords'], inplace=True)\n",
    "dataset = CustomDataset(data=data)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, collate_fn=CustomDataCollator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toal number of parameters: 971\n"
     ]
    }
   ],
   "source": [
    "model = VAEModel(9, 20, 1)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "paramerter = sum(n.numel() for n in model.parameters())\n",
    "print(f\"Toal number of parameters: {paramerter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|\u001b[32m          \u001b[0m| 0/20000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|\u001b[32m          \u001b[0m| 1/20000 [00:01<8:14:05,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 126748.8359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|\u001b[32m          \u001b[0m| 33/20000 [00:42<7:23:21,  1.33s/it]"
     ]
    }
   ],
   "source": [
    "num_epochs = 20000\n",
    "losses = []\n",
    "model.train()\n",
    "for epoch in tqdm(range(num_epochs), desc=\"Epochs\", colour=\"green\"):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        x_hat, mean, log_var = model(batch)\n",
    "        loss = network_loss(batch, x_hat, mean, log_var)\n",
    "        total_loss+=loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = total_loss/len(dataloader)\n",
    "    losses.append(epoch_loss)\n",
    "    if epoch%1000==0:\n",
    "        print(f\"Epoch: {epoch} Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 62.0000, 826.0000, 265.7000,  65.0000, 642.0000, 380.2000,   1.0000,\n",
       "           9.0000,   5.6000],\n",
       "        [ 21.0000, 778.0000, 305.4445,  18.0000, 610.0000, 279.3333,   0.0000,\n",
       "           9.0000,   4.5000]])"
      ]
     },
     "execution_count": 684,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.3107e-01, -2.3180e+05, -8.6308e+01, -1.1722e+05,  4.7765e+00,\n",
       "         -1.7863e+05,  2.4713e+05,  1.1046e+04, -3.7970e+01],\n",
       "        [ 6.7227e-01, -3.0429e+01, -3.1107e-01, -1.5859e+01, -6.0594e-01,\n",
       "         -2.3214e+01,  3.2139e+01,  7.7376e-01, -1.0839e-01]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 685,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_hat[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect = DetectAnomaly(losses)\n",
    "detect.chekcer(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python src/feature-pipeline/feature.py --bucket gs://electric-armor-395015-netlog-bucket --file_name_suffix \".json\" --netlog_bq_table \"electric-armor-395015.netlog_dataset.log\" --input_file_pattern '/anomaly/*.avro' --temp_location 'gs://electric-armor-395015-netlog-bucket/temp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install --upgrade pip\n",
    "# pip install datasets gcsfs torch polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker build -t hardeybisey/netlog-event-generator:latest .\n",
    "!docker push hardeybisey/netlog-event-generator:latest\n",
    "!python main.py --runner=DataflowRunner --project=electric-armor-395015 --region=europe-west2 --temp_location=gs://electric-armor-395015-netlog-bucket/tmp --sdk_container_image=hardeybisey/netlog-event-generator --topic=projects/electric-armor-395015/topics/netlog-topic --qps=1000 --event_type=anomaly --sdk_location=container --pickle_library=cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import avro.schema\n",
    "from avro.datafile import DataFileReader, DataFileWriter\n",
    "from avro.io import DatumReader, DatumWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subscription_name': 'gsc-sub', 'message_id': '8886770784205506', 'publish_time': datetime.datetime(2023, 10, 16, 10, 34, 20, 118000, tzinfo=<avro.timezones.UTCTzinfo object at 0x7fb92b52ba60>), 'attributes': {}, 'data': b'{\"subscriberId\": \"067b56b3-9cf2-4b20-a695-22e7d21d2198\", \"srcIP\": \"23.53.254.31\", \"srcPort\": 2781, \"dstIP\": \"10.104.26.160\", \"dstPort\": 3806, \"txBytes\": 372, \"rxBytes\": 275, \"startTime\": \"2023-10-16T10:34:19\", \"endTime\": \"2023-10-16T10:34:24\", \"tcpFlag\": \"PSH\", \"protocolName\": \"TCP\", \"protocolNumber\": 6}'}\n"
     ]
    }
   ],
   "source": [
    "reader = DataFileReader(open(\"/Users/hardey/Downloads/raw_2023-10-16T10 34 20+00 00_f4cc9c.avro\", \"rb\"), DatumReader())\n",
    "for user in reader:\n",
    "    break\n",
    "print(user)\n",
    "reader.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
