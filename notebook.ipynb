{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"REF: https://github.com/GoogleCloudPlatform/df-ml-anomaly-detection/blob/master/README.md\">Reference1<a/>  <br>\n",
    "<a href=\"https://faker.readthedocs.io/en/master/\">Reference2<a/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import ipaddress\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result(name='adam', sum=4)\n",
      "Result(name='basit', sum=6)\n",
      "Result(name='china', sum=5)\n",
      "Result(name='beijin', sum=6)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<apache_beam.runners.portability.fn_api_runner.fn_runner.RunnerResult at 0x7fb92d6cb640>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = beam.Pipeline()\n",
    "data = (p\n",
    "        | beam.Create([(\"adam\",1),(\"basit\",2),(\"adam\",3),(\"basit\",4),(\"china\",5),(\"beijin\",6)])\n",
    "        | beam.Map(lambda x: beam.Row(name=x[0],age=x[1]))\n",
    "        | beam.GroupBy(\"name\").aggregate_field(\"age\", sum, \"sum\")\n",
    "        | beam.Map(print))\n",
    "p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class WindowInto in module apache_beam.transforms.core:\n",
      "\n",
      "class WindowInto(ParDo)\n",
      " |  WindowInto(windowfn, trigger=None, accumulation_mode=None, timestamp_combiner=None, allowed_lateness=0)\n",
      " |  \n",
      " |  A window transform assigning windows to each element of a PCollection.\n",
      " |  \n",
      " |  Transforms an input PCollection by applying a windowing function to each\n",
      " |  element.  Each transformed element in the result will be a WindowedValue\n",
      " |  element with the same input value and timestamp, with its new set of windows\n",
      " |  determined by the windowing function.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      WindowInto\n",
      " |      ParDo\n",
      " |      apache_beam.transforms.ptransform.PTransformWithSideInputs\n",
      " |      apache_beam.transforms.ptransform.PTransform\n",
      " |      apache_beam.typehints.decorators.WithTypeHints\n",
      " |      apache_beam.transforms.display.HasDisplayData\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, windowfn, trigger=None, accumulation_mode=None, timestamp_combiner=None, allowed_lateness=0)\n",
      " |      Initializes a WindowInto transform.\n",
      " |      \n",
      " |      Args:\n",
      " |        windowfn (Windowing, WindowFn): Function to be used for windowing.\n",
      " |        trigger: (optional) Trigger used for windowing, or None for default.\n",
      " |        accumulation_mode: (optional) Accumulation mode used for windowing,\n",
      " |            required for non-trivial triggers.\n",
      " |        timestamp_combiner: (optional) Timestamp combniner used for windowing,\n",
      " |            or None for default.\n",
      " |  \n",
      " |  expand(self, pcoll)\n",
      " |  \n",
      " |  get_windowing(self, unused_inputs)\n",
      " |      Returns the window function to be associated with transform's output.\n",
      " |      \n",
      " |      By default most transforms just return the windowing function associated\n",
      " |      with the input PCollection (or the first input if several).\n",
      " |  \n",
      " |  infer_output_type(self, input_type)\n",
      " |  \n",
      " |  to_runner_api_parameter(self, context, **extra_kwargs)\n",
      " |      # typing: PTransform base class does not accept extra_kwargs\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_runner_api_parameter(unused_ptransform, proto, context)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  WindowIntoFn = <class 'apache_beam.transforms.core.WindowInto.WindowIn...\n",
      " |      A DoFn that applies a WindowInto operation.\n",
      " |  \n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ParDo:\n",
      " |  \n",
      " |  default_type_hints(self)\n",
      " |  \n",
      " |  display_data(self)\n",
      " |      Returns the display data associated to a pipeline component.\n",
      " |      \n",
      " |      It should be reimplemented in pipeline components that wish to have\n",
      " |      static display data.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dict[str, Any]: A dictionary containing ``key:value`` pairs.\n",
      " |        The value might be an integer, float or string value; a\n",
      " |        :class:`DisplayDataItem` for values that have more data\n",
      " |        (e.g. short value, label, url); or a :class:`HasDisplayData` instance\n",
      " |        that has more display data that should be picked up. For example::\n",
      " |      \n",
      " |          {\n",
      " |            'key1': 'string_value',\n",
      " |            'key2': 1234,\n",
      " |            'key3': 3.14159265,\n",
      " |            'key4': DisplayDataItem('apache.org', url='http://apache.org'),\n",
      " |            'key5': subComponent\n",
      " |          }\n",
      " |  \n",
      " |  get_restriction_coder(self)\n",
      " |      Returns `restriction coder if `DoFn` of this `ParDo` is a SDF.\n",
      " |      \n",
      " |      Returns `None` otherwise.\n",
      " |  \n",
      " |  infer_batch_converters(self, input_element_type)\n",
      " |  \n",
      " |  make_fn(self, fn, has_side_inputs)\n",
      " |  \n",
      " |  runner_api_requires_keyed_input(self)\n",
      " |  \n",
      " |  with_exception_handling(self, main_tag='good', dead_letter_tag='bad', *, exc_class=<class 'Exception'>, partial=False, use_subprocess=False, threshold=1, threshold_windowing=None, timeout=None)\n",
      " |      Automatically provides a dead letter output for skipping bad records.\n",
      " |      This can allow a pipeline to continue successfully rather than fail or\n",
      " |      continuously throw errors on retry when bad elements are encountered.\n",
      " |      \n",
      " |      This returns a tagged output with two PCollections, the first being the\n",
      " |      results of successfully processing the input PCollection, and the second\n",
      " |      being the set of bad records (those which threw exceptions during\n",
      " |      processing) along with information about the errors raised.\n",
      " |      \n",
      " |      For example, one would write::\n",
      " |      \n",
      " |          good, bad = Map(maybe_error_raising_function).with_exception_handling()\n",
      " |      \n",
      " |      and `good` will be a PCollection of mapped records and `bad` will contain\n",
      " |      those that raised exceptions.\n",
      " |      \n",
      " |      \n",
      " |      Args:\n",
      " |        main_tag: tag to be used for the main (good) output of the DoFn,\n",
      " |            useful to avoid possible conflicts if this DoFn already produces\n",
      " |            multiple outputs.  Optional, defaults to 'good'.\n",
      " |        dead_letter_tag: tag to be used for the bad records, useful to avoid\n",
      " |            possible conflicts if this DoFn already produces multiple outputs.\n",
      " |            Optional, defaults to 'bad'.\n",
      " |        exc_class: An exception class, or tuple of exception classes, to catch.\n",
      " |            Optional, defaults to 'Exception'.\n",
      " |        partial: Whether to emit outputs for an element as they're produced\n",
      " |            (which could result in partial outputs for a ParDo or FlatMap that\n",
      " |            throws an error part way through execution) or buffer all outputs\n",
      " |            until successful processing of the entire element. Optional,\n",
      " |            defaults to False.\n",
      " |        use_subprocess: Whether to execute the DoFn logic in a subprocess. This\n",
      " |            allows one to recover from errors that can crash the calling process\n",
      " |            (e.g. from an underlying C/C++ library causing a segfault), but is\n",
      " |            slower as elements and results must cross a process boundary.  Note\n",
      " |            that this starts up a long-running process that is used to handle\n",
      " |            all the elements (until hard failure, which should be rare) rather\n",
      " |            than a new process per element, so the overhead should be minimal\n",
      " |            (and can be amortized if there's any per-process or per-bundle\n",
      " |            initialization that needs to be done). Optional, defaults to False.\n",
      " |        threshold: An upper bound on the ratio of records that can be bad before\n",
      " |            aborting the entire pipeline. Optional, defaults to 1.0 (meaning\n",
      " |            up to 100% of records can be bad and the pipeline will still succeed).\n",
      " |        threshold_windowing: Event-time windowing to use for threshold. Optional,\n",
      " |            defaults to the windowing of the input.\n",
      " |        timeout: If the element has not finished processing in timeout seconds,\n",
      " |            raise a TimeoutError.  Defaults to None, meaning no time limit.\n",
      " |  \n",
      " |  with_outputs(self, *tags, main=None, allow_unknown_tags=None)\n",
      " |      Returns a tagged tuple allowing access to the outputs of a\n",
      " |      :class:`ParDo`.\n",
      " |      \n",
      " |      The resulting object supports access to the\n",
      " |      :class:`~apache_beam.pvalue.PCollection` associated with a tag\n",
      " |      (e.g. ``o.tag``, ``o[tag]``) and iterating over the available tags\n",
      " |      (e.g. ``for tag in o: ...``).\n",
      " |      \n",
      " |      Args:\n",
      " |        *tags: if non-empty, list of valid tags. If a list of valid tags is given,\n",
      " |          it will be an error to use an undeclared tag later in the pipeline.\n",
      " |        **main_kw: dictionary empty or with one key ``'main'`` defining the tag to\n",
      " |          be used for the main output (which will not have a tag associated with\n",
      " |          it).\n",
      " |      \n",
      " |      Returns:\n",
      " |        ~apache_beam.pvalue.DoOutputsTuple: An object of type\n",
      " |        :class:`~apache_beam.pvalue.DoOutputsTuple` that bundles together all\n",
      " |        the outputs of a :class:`ParDo` transform and allows accessing the\n",
      " |        individual :class:`~apache_beam.pvalue.PCollection` s for each output\n",
      " |        using an ``object.tag`` syntax.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if the **self** object is not a\n",
      " |          :class:`~apache_beam.pvalue.PCollection` that is the result of a\n",
      " |          :class:`ParDo` transform.\n",
      " |        ValueError: if **main_kw** contains any key other than\n",
      " |          ``'main'``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from apache_beam.transforms.ptransform.PTransformWithSideInputs:\n",
      " |  \n",
      " |  default_label(self)\n",
      " |  \n",
      " |  type_check_inputs(self, pvalueish)\n",
      " |  \n",
      " |  with_input_types(self, input_type_hint, *side_inputs_arg_hints, **side_input_kwarg_hints)\n",
      " |      Annotates the types of main inputs and side inputs for the PTransform.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_type_hint: An instance of an allowed built-in type, a custom class,\n",
      " |          or an instance of a typehints.TypeConstraint.\n",
      " |        *side_inputs_arg_hints: A variable length argument composed of\n",
      " |          of an allowed built-in type, a custom class, or a\n",
      " |          typehints.TypeConstraint.\n",
      " |        **side_input_kwarg_hints: A dictionary argument composed of\n",
      " |          of an allowed built-in type, a custom class, or a\n",
      " |          typehints.TypeConstraint.\n",
      " |      \n",
      " |      Example of annotating the types of side-inputs::\n",
      " |      \n",
      " |        FlatMap().with_input_types(int, int, bool)\n",
      " |      \n",
      " |      Raises:\n",
      " |        :class:`TypeError`: If **type_hint** is not a valid type-hint.\n",
      " |          See\n",
      " |          :func:`~apache_beam.typehints.typehints.validate_composite_type_param`\n",
      " |          for further details.\n",
      " |      \n",
      " |      Returns:\n",
      " |        :class:`PTransform`: A reference to the instance of this particular\n",
      " |        :class:`PTransform` object. This allows chaining type-hinting related\n",
      " |        methods.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from apache_beam.transforms.ptransform.PTransform:\n",
      " |  \n",
      " |  __or__(self, right)\n",
      " |      Used to compose PTransforms, e.g., ptransform1 | ptransform2.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __ror__(self, left, label=None)\n",
      " |      Used to apply this PTransform to non-PValues, e.g., a tuple.\n",
      " |  \n",
      " |  __rrshift__(self, label)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  annotations(self) -> Dict[str, Union[bytes, str, google.protobuf.message.Message]]\n",
      " |  \n",
      " |  get_resource_hints(self)\n",
      " |  \n",
      " |  to_runner_api(self, context, has_parts=False, **extra_kwargs)\n",
      " |  \n",
      " |  to_runner_api_pickled(self, unused_context)\n",
      " |  \n",
      " |  type_check_inputs_or_outputs(self, pvalueish, input_or_output)\n",
      " |  \n",
      " |  type_check_outputs(self, pvalueish)\n",
      " |  \n",
      " |  with_output_types(self, type_hint)\n",
      " |      Annotates the output type of a :class:`PTransform` with a type-hint.\n",
      " |      \n",
      " |      Args:\n",
      " |        type_hint (type): An instance of an allowed built-in type, a custom class,\n",
      " |          or a :class:`~apache_beam.typehints.typehints.TypeConstraint`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If **type_hint** is not a valid type-hint. See\n",
      " |          :obj:`~apache_beam.typehints.typehints.validate_composite_type_param()`\n",
      " |          for further details.\n",
      " |      \n",
      " |      Returns:\n",
      " |        PTransform: A reference to the instance of this particular\n",
      " |        :class:`PTransform` object. This allows chaining type-hinting related\n",
      " |        methods.\n",
      " |  \n",
      " |  with_resource_hints(self, **kwargs)\n",
      " |      Adds resource hints to the :class:`PTransform`.\n",
      " |      \n",
      " |      Resource hints allow users to express constraints on the environment where\n",
      " |      the transform should be executed.  Interpretation of the resource hints is\n",
      " |      defined by Beam Runners. Runners may ignore the unsupported hints.\n",
      " |      \n",
      " |      Args:\n",
      " |        **kwargs: key-value pairs describing hints and their values.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if provided hints are unknown to the SDK. See\n",
      " |          :mod:`apache_beam.transforms.resources` for a list of known hints.\n",
      " |      \n",
      " |      Returns:\n",
      " |        PTransform: A reference to the instance of this particular\n",
      " |        :class:`PTransform` object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from apache_beam.transforms.ptransform.PTransform:\n",
      " |  \n",
      " |  from_runner_api(proto, context) from builtins.type\n",
      " |  \n",
      " |  register_urn(urn, parameter_type, constructor=None) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from apache_beam.transforms.ptransform.PTransform:\n",
      " |  \n",
      " |  label\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from apache_beam.transforms.ptransform.PTransform:\n",
      " |  \n",
      " |  __orig_bases__ = (<class 'apache_beam.typehints.decorators.WithTypeHin...\n",
      " |  \n",
      " |  pipeline = None\n",
      " |  \n",
      " |  side_inputs = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from apache_beam.typehints.decorators.WithTypeHints:\n",
      " |  \n",
      " |  get_type_hints(self)\n",
      " |      Gets and/or initializes type hints for this object.\n",
      " |      \n",
      " |      If type hints have not been set, attempts to initialize type hints in this\n",
      " |      order:\n",
      " |      - Using self.default_type_hints().\n",
      " |      - Using self.__class__ type hints.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from apache_beam.typehints.decorators.WithTypeHints:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(beam.WindowInto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "\n",
    "def ip_to_cidr(ip, subnet_mask):\n",
    "    # Parse the IP address and subnet mask\n",
    "    ip_network = ipaddress.IPv4Network(f\"{ip}/{subnet_mask}\", strict=False)\n",
    "    \n",
    "    # Get the CIDR notation\n",
    "    cidr_notation = str(ip_network)\n",
    "    \n",
    "    return cidr_notation\n",
    "\n",
    "# Example usage\n",
    "ip = \"192.168.1.10\"\n",
    "subnet_mask = \"255.255.255.0\"\n",
    "ip = ipaddress.IPv4Address(ip)\n",
    "cidr = ip_to_cidr(ip, subnet_mask)\n",
    "print(cidr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 pipeline.py --streaming --project \"electric-armor-395015\" --topic \"projects/electric-armor-395015/topics/test\" --qps 10 --runner \"DataflowRunner\" --region europe-west2 temp_location gs://my_terraform_state_bucket/temp staging_location gs://my_terraform_state_bucket/staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud pubsub subscriptions pull test-sub --auto-ack --limit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam.io.WriteToText(\"gs://electric-armor-395015-netlog-bucket/test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python src/data-pipeline/main.py --topic projects/electric-armor-395015/topics/${TOPIC_NAME} --qps 100 --anomaly True --runner TestDataFlowRunner --sdk_container_image gcr.io/electric-armor-395015/netlog-stream-template:${TAG} --project electric-armor-395015 --region europe-west2 --temp_location gs://${BUCKET}/tmp --staging_location gs://${BUCKET}/staging --job_name netlog-stream-job --max_num_workers 1 --worker_machine_type n1-standard-1 --job_name test23456789"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.size(np.array([1,2,3]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
