{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"REF: https://github.com/GoogleCloudPlatform/df-ml-anomaly-detection/blob/master/README.md\">Reference1<a/>  <br>\n",
    "<a href=\"https://faker.readthedocs.io/en/master/\">Reference2<a/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniqueCombine(beam.CombineFn):\n",
    "    def create_accumulator(self):\n",
    "        return set()\n",
    "    \n",
    "    def add_input(self, accumulator, element):\n",
    "        accumulator.add(element)\n",
    "        return accumulator\n",
    "    \n",
    "    def merge_accumulators(self, accumulators):\n",
    "        return set.union(*accumulators)\n",
    "    \n",
    "    def extract_output(self, accumulator):\n",
    "        return len(accumulator)\n",
    "    \n",
    "def pick(n):\n",
    "    return n[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    total_unique_elements = (\n",
    "        pipeline\n",
    "        | 'Create produce' >> beam.Create(\n",
    "            [{\"name\":\"idris\",\"salary\":500,\"test\":5}, {\"name\":\"idris\",\"salary\":500,\"test\":5}, {\"name\":\"idris\",\"salary\":500,\"test\":6}, {\"name\":\"ade\",\"salary\":900,\"test\":5}])\n",
    "        | 'row' >> beam.Map(lambda row: beam.Row(name=row['name'],salary=row[\"salary\"],test=row[\"test\"]))\n",
    "        | 'Pair with 1' >> beam.GroupBy(\"name\", \"salary\")\\\n",
    "        .aggregate_field(\n",
    "            \"name\", UniqueCombine(), \"count\"\n",
    "        ).aggregate_field(\n",
    "            \"test\", UniqueCombine(), \"test\"\n",
    "        ).aggregate_field(\n",
    "            \"name\", pick, \"avg_salary\"\n",
    "        )\n",
    "        )\n",
    "    total_unique_elements | 'Add timestamp' >> beam.ParDo(AddTimestamp()).with_output_types(r) | beam.Map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with beam.Pipeline() as pipeline:\n",
    "    total_unique_elements = (\n",
    "        pipeline\n",
    "        | 'Create produce' >> beam.Create(\n",
    "            [{\"name\":\"idris\",\"salary\":500,\"test\":5}, {\"name\":\"idris\",\"salary\":500,\"test\":5}, {\"name\":\"idris\",\"salary\":500,\"test\":6}, {\"name\":\"ade\",\"salary\":900,\"test\":5}])\n",
    "        | 'row' >> beam.Map(lambda row: beam.Row(name=row['name'],salary=row[\"salary\"],test=row[\"test\"]))\n",
    "        )\n",
    "    total_unique_elements | 'Add timestamp' >> beam.ParDo(AddTimestamp()) | beam.Map(print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "faker = Faker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maskString = \"255.255.252.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "\n",
    "def ip_to_cidr(ip, subnet_mask):\n",
    "    # Parse the IP address and subnet mask\n",
    "    ip_network = ipaddress.IPv4Network(f\"{ip}/{subnet_mask}\", strict=False)\n",
    "    \n",
    "    # Get the CIDR notation\n",
    "    cidr_notation = str(ip_network)\n",
    "    \n",
    "    return cidr_notation\n",
    "\n",
    "# Example usage\n",
    "ip = \"192.168.1.10\"\n",
    "subnet_mask = \"255.255.255.0\"\n",
    "\n",
    "cidr = ip_to_cidr(ip, subnet_mask)\n",
    "print(cidr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = ipaddress.IPv4Address(ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = beam.Pipeline()\n",
    "raw_events = (\n",
    "    pipeline\n",
    "    | 'Read from PubSub' >> beam.Create(['{\"foo\": \"bar\"}' for _ in range(10)])\n",
    "    | 'Parse events' >> beam.ParDo(EventParser()).with_outputs('valid', 'invalid')\n",
    ")\n",
    "\n",
    "(raw_events.invalid \n",
    " |'Log invalid events' >> beam.io.fileio.WriteToFiles(\"data/invalid_events\",shards=2,max_writers_per_bundle=2))\n",
    "\n",
    "(raw_events.valid \n",
    " | \"time\" >> beam.Map(lambda x: beam.window.TimestampedValue(x, time.time()))\n",
    " | 'Add timestamps' >> beam.ParDo(AddTimestamp())\n",
    " | \"add key\" >> beam.Map(print))\n",
    "\n",
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import apache_beam as beam\n",
    "from utils.custom import NetworkPool, UserObject, JsonEvent\n",
    "from apache_beam.transforms.periodicsequence import PeriodicImpulse\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "        \n",
    "def to_json(event):\n",
    "    return json.dumps(event).encode('utf-8')        \n",
    "\n",
    "class AgggregateEvent(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        \n",
    "        network = self.network_pool.get_network()\n",
    "        user = self.user_obj.get_user()\n",
    "        num_events = random.randint(5, self.max_events_per_session)\n",
    "        for _ in range(num_events):\n",
    "            event = JsonEvent.generate(user, network)\n",
    "            yield event\n",
    "                        \n",
    "def run(args, beam_args):\n",
    "    options = PipelineOptions(beam_args, save_main_session=True, streaming=True)\n",
    "    pipeline = beam.Pipeline(options=options)\n",
    "    (\n",
    "        pipeline\n",
    "        | \"Trigger\" >> PeriodicImpulse(start_timestamp=time.time(), fire_interval=(60/args.qps))\n",
    "        | \"Generate Events\" >> beam.ParDo(EventGenerator())\n",
    "        | \"JSONIFY\" >> beam.Map(to_json)\n",
    "        | \"Write to PubSub\" >> beam.io.WriteToPubSub(args.topic)\n",
    "    )\n",
    "    return pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "\n",
    "{\n",
    "  \"subscriberId\":(11111111111,99999999999)\n",
    "  \"srcIP\": \"ipv4\"\n",
    "  \"dstIP\": \"subnet\"\n",
    "  \"srcPort\": (1000,5000)\n",
    "  \"dstPort\": (1000,5000)\n",
    "  \"txBytes\": (0,1000000000)\n",
    "  \"rxBytes\": (0,1000000000)\n",
    "  \"tcpFlag\": (0,65)\n",
    "  \"startTime\": \"timestamp\"\n",
    "  \"endTime\": \"timestamp\"\n",
    "  \"protocolName\": \"tcp|udp|http\"\n",
    "  \"protocolNumber\": (0,255)\n",
    "  \"geoCountry\": \"country\"\n",
    "  \"geoCity\": \"city\"\n",
    "  \"latitude\": (0,90)\n",
    "  \"longitude\": (0,180)\n",
    "}\n",
    "\n",
    "# transformed data\n",
    "{\n",
    "  \"transaction_time\": \"2019-10-27 23:22:17.848000\",\n",
    "  \"subscriber_id\": \"100\",\n",
    "  \"dst_subnet\": \"12.0.1.2/22\",\n",
    "  \"number_of_unique_ips\": \"1\",\n",
    "  \"number_of_unique_ports\": \"1\",\n",
    "  \"number_of_records\": \"2\",\n",
    "  \"max_tx_bytes\": \"15\",\n",
    "  \"min_tx_bytes\": \"10\",\n",
    "  \"avg_tx_bytes\": \"12.5\",\n",
    "  \"max_rx_bytes\": \"40\",\n",
    "  \"min_rx_bytes\": \"40\",\n",
    "  \"avg_rx_bytes\": \"40.0\",\n",
    "  \"max_duration\": \"100\",\n",
    "  \"min_duration\": \"9\",\n",
    "  \"avg_duration\": \"54.5\"\n",
    "}\n",
    "\n",
    "# model features\n",
    "{  \"number_of_records\": \"2\",\n",
    "  \"max_tx_bytes\": \"15\",\n",
    "  \"min_tx_bytes\": \"10\",\n",
    "  \"avg_tx_bytes\": \"12.5\",\n",
    "  \"max_rx_bytes\": \"40\",\n",
    "  \"min_rx_bytes\": \"40\",\n",
    "  \"avg_rx_bytes\": \"40.0\",\n",
    "  \"max_duration\": \"100\",\n",
    "  \"min_duration\": \"9\",\n",
    "  \"avg_duration\": \"54.5\"\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python3 pipeline.py --streaming --project \"electric-armor-395015\" --topic \"projects/electric-armor-395015/topics/test\" --qps 10 --runner \"DataflowRunner\" --region europe-west2 temp_location gs://my_terraform_state_bucket/temp staging_location gs://my_terraform_state_bucket/staging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gcloud pubsub subscriptions pull test-sub --auto-ack --limit 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ReadFromPubSub in module apache_beam.io.gcp.pubsub:\n",
      "\n",
      "class ReadFromPubSub(apache_beam.transforms.ptransform.PTransform)\n",
      " |  ReadFromPubSub(topic=None, subscription=None, id_label=None, with_attributes=False, timestamp_attribute=None)\n",
      " |  \n",
      " |  A ``PTransform`` for reading from Cloud Pub/Sub.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ReadFromPubSub\n",
      " |      apache_beam.transforms.ptransform.PTransform\n",
      " |      apache_beam.typehints.decorators.WithTypeHints\n",
      " |      apache_beam.transforms.display.HasDisplayData\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, topic=None, subscription=None, id_label=None, with_attributes=False, timestamp_attribute=None)\n",
      " |      Initializes ``ReadFromPubSub``.\n",
      " |      \n",
      " |      Args:\n",
      " |        topic: Cloud Pub/Sub topic in the form\n",
      " |          \"projects/<project>/topics/<topic>\". If provided, subscription must be\n",
      " |          None.\n",
      " |        subscription: Existing Cloud Pub/Sub subscription to use in the\n",
      " |          form \"projects/<project>/subscriptions/<subscription>\". If not\n",
      " |          specified, a temporary subscription will be created from the specified\n",
      " |          topic. If provided, topic must be None.\n",
      " |        id_label: The attribute on incoming Pub/Sub messages to use as a unique\n",
      " |          record identifier. When specified, the value of this attribute (which\n",
      " |          can be any string that uniquely identifies the record) will be used for\n",
      " |          deduplication of messages. If not provided, we cannot guarantee\n",
      " |          that no duplicate data will be delivered on the Pub/Sub stream. In this\n",
      " |          case, deduplication of the stream will be strictly best effort.\n",
      " |        with_attributes:\n",
      " |          True - output elements will be :class:`~PubsubMessage` objects.\n",
      " |          False - output elements will be of type ``bytes`` (message\n",
      " |          data only).\n",
      " |        timestamp_attribute: Message value to use as element timestamp. If None,\n",
      " |          uses message publishing time as the timestamp.\n",
      " |      \n",
      " |          Timestamp values should be in one of two formats:\n",
      " |      \n",
      " |          - A numerical value representing the number of milliseconds since the\n",
      " |            Unix epoch.\n",
      " |          - A string in RFC 3339 format, UTC timezone. Example:\n",
      " |            ``2015-10-29T23:41:41.123Z``. The sub-second component of the\n",
      " |            timestamp is optional, and digits beyond the first three (i.e., time\n",
      " |            units smaller than milliseconds) may be ignored.\n",
      " |  \n",
      " |  expand(self, pvalue)\n",
      " |  \n",
      " |  to_runner_api_parameter(self, context)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __parameters__ = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from apache_beam.transforms.ptransform.PTransform:\n",
      " |  \n",
      " |  __or__(self, right)\n",
      " |      Used to compose PTransforms, e.g., ptransform1 | ptransform2.\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __ror__(self, left, label=None)\n",
      " |      Used to apply this PTransform to non-PValues, e.g., a tuple.\n",
      " |  \n",
      " |  __rrshift__(self, label)\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  annotations(self) -> Dict[str, Union[bytes, str, google.protobuf.message.Message]]\n",
      " |  \n",
      " |  default_label(self)\n",
      " |  \n",
      " |  default_type_hints(self)\n",
      " |  \n",
      " |  get_resource_hints(self)\n",
      " |  \n",
      " |  get_windowing(self, inputs)\n",
      " |      Returns the window function to be associated with transform's output.\n",
      " |      \n",
      " |      By default most transforms just return the windowing function associated\n",
      " |      with the input PCollection (or the first input if several).\n",
      " |  \n",
      " |  infer_output_type(self, unused_input_type)\n",
      " |  \n",
      " |  runner_api_requires_keyed_input(self)\n",
      " |  \n",
      " |  to_runner_api(self, context, has_parts=False, **extra_kwargs)\n",
      " |  \n",
      " |  to_runner_api_pickled(self, unused_context)\n",
      " |  \n",
      " |  type_check_inputs(self, pvalueish)\n",
      " |  \n",
      " |  type_check_inputs_or_outputs(self, pvalueish, input_or_output)\n",
      " |  \n",
      " |  type_check_outputs(self, pvalueish)\n",
      " |  \n",
      " |  with_input_types(self, input_type_hint)\n",
      " |      Annotates the input type of a :class:`PTransform` with a type-hint.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_type_hint (type): An instance of an allowed built-in type, a custom\n",
      " |          class, or an instance of a\n",
      " |          :class:`~apache_beam.typehints.typehints.TypeConstraint`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If **input_type_hint** is not a valid type-hint.\n",
      " |          See\n",
      " |          :obj:`apache_beam.typehints.typehints.validate_composite_type_param()`\n",
      " |          for further details.\n",
      " |      \n",
      " |      Returns:\n",
      " |        PTransform: A reference to the instance of this particular\n",
      " |        :class:`PTransform` object. This allows chaining type-hinting related\n",
      " |        methods.\n",
      " |  \n",
      " |  with_output_types(self, type_hint)\n",
      " |      Annotates the output type of a :class:`PTransform` with a type-hint.\n",
      " |      \n",
      " |      Args:\n",
      " |        type_hint (type): An instance of an allowed built-in type, a custom class,\n",
      " |          or a :class:`~apache_beam.typehints.typehints.TypeConstraint`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If **type_hint** is not a valid type-hint. See\n",
      " |          :obj:`~apache_beam.typehints.typehints.validate_composite_type_param()`\n",
      " |          for further details.\n",
      " |      \n",
      " |      Returns:\n",
      " |        PTransform: A reference to the instance of this particular\n",
      " |        :class:`PTransform` object. This allows chaining type-hinting related\n",
      " |        methods.\n",
      " |  \n",
      " |  with_resource_hints(self, **kwargs)\n",
      " |      Adds resource hints to the :class:`PTransform`.\n",
      " |      \n",
      " |      Resource hints allow users to express constraints on the environment where\n",
      " |      the transform should be executed.  Interpretation of the resource hints is\n",
      " |      defined by Beam Runners. Runners may ignore the unsupported hints.\n",
      " |      \n",
      " |      Args:\n",
      " |        **kwargs: key-value pairs describing hints and their values.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if provided hints are unknown to the SDK. See\n",
      " |          :mod:`apache_beam.transforms.resources` for a list of known hints.\n",
      " |      \n",
      " |      Returns:\n",
      " |        PTransform: A reference to the instance of this particular\n",
      " |        :class:`PTransform` object.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from apache_beam.transforms.ptransform.PTransform:\n",
      " |  \n",
      " |  from_runner_api(proto, context) from builtins.type\n",
      " |  \n",
      " |  register_urn(urn, parameter_type, constructor=None) from builtins.type\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from apache_beam.transforms.ptransform.PTransform:\n",
      " |  \n",
      " |  label\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from apache_beam.transforms.ptransform.PTransform:\n",
      " |  \n",
      " |  __orig_bases__ = (<class 'apache_beam.typehints.decorators.WithTypeHin...\n",
      " |  \n",
      " |  pipeline = None\n",
      " |  \n",
      " |  side_inputs = ()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from apache_beam.typehints.decorators.WithTypeHints:\n",
      " |  \n",
      " |  get_type_hints(self)\n",
      " |      Gets and/or initializes type hints for this object.\n",
      " |      \n",
      " |      If type hints have not been set, attempts to initialize type hints in this\n",
      " |      order:\n",
      " |      - Using self.default_type_hints().\n",
      " |      - Using self.__class__ type hints.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from apache_beam.typehints.decorators.WithTypeHints:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from apache_beam.transforms.display.HasDisplayData:\n",
      " |  \n",
      " |  display_data(self)\n",
      " |      Returns the display data associated to a pipeline component.\n",
      " |      \n",
      " |      It should be reimplemented in pipeline components that wish to have\n",
      " |      static display data.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Dict[str, Any]: A dictionary containing ``key:value`` pairs.\n",
      " |        The value might be an integer, float or string value; a\n",
      " |        :class:`DisplayDataItem` for values that have more data\n",
      " |        (e.g. short value, label, url); or a :class:`HasDisplayData` instance\n",
      " |        that has more display data that should be picked up. For example::\n",
      " |      \n",
      " |          {\n",
      " |            'key1': 'string_value',\n",
      " |            'key2': 1234,\n",
      " |            'key3': 3.14159265,\n",
      " |            'key4': DisplayDataItem('apache.org', url='http://apache.org'),\n",
      " |            'key5': subComponent\n",
      " |          }\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __class_getitem__(params) from builtins.type\n",
      " |  \n",
      " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(beam.io.ReadFromPubSub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from faker import Faker\n",
    "from typing import NamedTuple\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "client = Faker()\n",
    "\n",
    "class Network(NamedTuple):\n",
    "    ipv4: str\n",
    "    port: int\n",
    "    protocol_name: str\n",
    "    protocol_num: int\n",
    "    \n",
    "class User(NamedTuple):\n",
    "    subscriber_id: str\n",
    "    ipv4: str\n",
    "    port: int\n",
    "\n",
    "class NetworkPool:\n",
    "    def __init__(self, num_dest_ip=10000):\n",
    "        self._client = client\n",
    "        self._num_dest_ip = num_dest_ip\n",
    "        self._protocols = [(\"TCP\", 6), (\"UDP\", 17), (\"HTTP\", 80), (\"HTTPS\", 443)]\n",
    "        self._ip = self._generate_ipv4()\n",
    "        \n",
    "    def _ipv4(self):\n",
    "        return random.choice(self._ip)\n",
    "     \n",
    "    def _port(self):\n",
    "        return random.randint(1000, 5000)\n",
    "    \n",
    "    def _protocol_info(self):\n",
    "        return random.choice(self._protocols)\n",
    "    \n",
    "    def _generate_ipv4(self):\n",
    "        return [self._client.ipv4_private() for _ in range(self._num_dest_ip)]\n",
    "    \n",
    "    def get_network(self):\n",
    "        protocol = self._protocol_info()\n",
    "        return Network(self._ipv4(), self._port(), protocol[0], protocol[1])\n",
    "\n",
    "class UserObject:\n",
    "    def __init__(self):\n",
    "        self.client = client\n",
    "        \n",
    "    def _subscriber_id(self):\n",
    "        return self.client.uuid4()\n",
    "    \n",
    "    def _ipv4(self):\n",
    "        return self.client.ipv4()\n",
    "    \n",
    "    def _port(self):\n",
    "        return random.randint(1000, 5000)\n",
    "    \n",
    "    def get_user(self):\n",
    "        return User(self._subscriber_id(), self._ipv4(), self._port())\n",
    "    \n",
    "class JsonEvent:\n",
    "    max_request_bytes = 5000\n",
    "    allowed_lag_sec = 10\n",
    "    tcp_flag = [\"SYN\", \"ACK\", \"FIN\", \"RST\", \"PSH\", \"URG\"]\n",
    "    \n",
    "    @classmethod\n",
    "    def generate(cls, user, network, anomaly):\n",
    "        start_time = datetime.now()\n",
    "        time_diff = random.uniform(0, cls.allowed_lag_sec)\n",
    "        end_time = start_time + timedelta(seconds=time_diff)\n",
    "        return {\"subscriberId\": user.subscriber_id,\n",
    "                \"srcIP\": user.ipv4,\n",
    "                \"srcPort\": user.port,\n",
    "                \"dstIP\": network.ipv4,\n",
    "                \"dstPort\": network.port,\n",
    "                \"txBytes\": cls._normalized_bytes(time_diff, anomaly),\n",
    "                \"rxBytes\": cls._normalized_bytes(time_diff, anomaly),\n",
    "                \"startTime\": start_time.isoformat(),\n",
    "                \"endTime\": end_time.isoformat(),\n",
    "                \"tcpFlag\": random.choice(cls.tcp_flag),\n",
    "                \"protocolName\": network.protocol_name,\n",
    "                \"protocolNumber\": network.protocol_num}\n",
    "        \n",
    "    @classmethod\n",
    "    def _normalized_bytes(cls, lag_time, anomaly):\n",
    "        random_byte = random.uniform(10, 100)\n",
    "        normalized_byte = int(min((random_byte * lag_time), cls.max_request_bytes))\n",
    "        return normalized_byte if not anomaly else normalized_byte * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import apache_beam as beam\n",
    "# from utils.custom import NetworkPool, UserObject, JsonEvent\n",
    "from apache_beam.transforms.periodicsequence import PeriodicImpulse\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class EventGenerator(beam.DoFn):\n",
    "    def __init__(self, anomaly, max_events_per_session=20, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.max_events_per_session = max_events_per_session\n",
    "        self.anomaly = anomaly\n",
    "        self.network_pool = NetworkPool()\n",
    "    \n",
    "    def setup(self):\n",
    "        self.user_obj = UserObject()    \n",
    "\n",
    "    def process(self, element):\n",
    "        network = self.network_pool.get_network()\n",
    "        user = self.user_obj.get_user()\n",
    "        num_events = random.randint(5, self.max_events_per_session)\n",
    "        logger.debug(f\"Generating {num_events} events for user {user.subscriber_id} on network {network.ipv4}\")\n",
    "        for _ in range(num_events):\n",
    "            event = JsonEvent.generate(user, network, self.anomaly)\n",
    "            yield event\n",
    "\n",
    "def to_json(event):\n",
    "    return json.dumps(event).encode('utf-8')\n",
    "\n",
    "def run():\n",
    "    options = PipelineOptions(save_main_session=True, streaming=True)\n",
    "    pipeline = beam.Pipeline()\n",
    "    (\n",
    "        pipeline\n",
    "        | \"Trigger\" >> PeriodicImpulse(fire_interval=(60/5))\n",
    "        | \"Generate Events\" >> beam.ParDo(EventGenerator(anomaly=True))\n",
    "        | \"JSONIFY\" >> beam.Map(to_json)\n",
    "        | \"Write to PubSub\" >> beam.Map(print)\n",
    "    )\n",
    "    return pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "# Define the PubSub subscription and GCS bucket\n",
    "subscription = \"projects/my-project/subscriptions/my-subscription\"\n",
    "bucket = \"gs://my-bucket\"\n",
    "\n",
    "# Define the Cloud Dataflow pipeline\n",
    "pipeline = beam.Pipeline(runner=\"DirectRunner\")\n",
    "\n",
    "# Read from PubSub and write to GCS\n",
    "(pipeline\n",
    "  | \"Read from PubSub\" >> beam.io.gcp.pubsub.ReadFromPubSub(subscription=subscription)\n",
    "  | \"Transform data\" >> beam.ParDo(DoFn())\n",
    "  | \"Write to GCS\" >> beam.io.WriteToText(bucket))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
